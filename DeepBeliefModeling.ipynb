{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# Deep Belief Modeling\n",
    "Project for SI course"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Imports"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "import imageio as imageio\n",
    "import pandas as pd"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Loading data"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "df_train = pd.read_csv(\"resources/data.csv\", sep=\"\\t\")\n",
    "# maybe add test data in future"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Input data visualisation"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "x = df_train[\"label\"].value_counts()\n",
    "x.plot(kind=\"bar\", color=[\"r\", \"b\"])"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Text pre-processing for classification"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "from preprocessing.article import word_preprocess"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "print(df_train[\"data\"].head(10))\n",
    "print(df_train[\"data\"].head(10).apply(word_preprocess))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Feature selection"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### TF-IDF Vectorizer"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# https://medium.com/@cmukesh8688/tf-idf-vectorizer-scikit-learn-dbc0244a911a\n",
    "import pickle\n",
    "\n",
    "with open(\"model_storage/vectorizer.pkl\",\"rb\") as file:\n",
    "    tfidf_vectorizer = pickle.load(file)\n",
    "\n",
    "with open(\"model_storage/bow.pkl\",\"rb\") as file:\n",
    "    tfidf_wm = pickle.load(file)\n",
    "\n",
    "with open(\"model_storage/classifier.pkl\",\"rb\") as file:\n",
    "    clf = pickle.load(file)\n",
    "\n",
    "tfidf_tokens = tfidf_vectorizer.get_feature_names_out()\n",
    "\n",
    "print(tfidf_tokens)\n",
    "\n",
    "df_tfidf_vect = pd.DataFrame(data=tfidf_wm.toarray(), columns=tfidf_tokens)\n",
    "\n",
    "print(\"Tf-idf vectorizer\")\n",
    "print(df_tfidf_vect)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "from matplotlib import pyplot as plt\n",
    "from sklearn.decomposition import TruncatedSVD\n",
    "\n",
    "clftr = TruncatedSVD(2)\n",
    "Xpca = clftr.fit_transform(tfidf_wm)\n",
    "plt.scatter([x[0] for x in Xpca],[x[1] for x in Xpca])\n",
    "plt.show()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "from random import randint\n",
    "from sklearn.datasets import load_iris\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.svm import SVC\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.colors import ListedColormap\n",
    "import numpy as np\n",
    "h = .01\n",
    "x_min, x_max = -0.4,0.6\n",
    "y_min, y_max = -0.4,0.6\n",
    "# loading dataset\n",
    "data = load_iris()\n",
    "X, y = tfidf_wm.toarray(), df_train[\"label\"]\n",
    "# selecting first 2 components of PCA\n",
    "X_selected = PCA(2).fit_transform(X)\n",
    "# training classifier and evaluating on the whole plane\n",
    "clfsvc = SVC()\n",
    "clfsvc.fit(X_selected,y)\n",
    "xx, yy = np.meshgrid(np.arange(x_min, x_max, h), np.arange(y_min, y_max, h))\n",
    "Z = clfsvc.predict(np.c_[xx.ravel(), yy.ravel()])\n",
    "Z = Z.reshape(xx.shape)\n",
    "# Plotting\n",
    "cmap_light = ListedColormap(['#FFAAAA', '#AAFFAA', '#AAAAFF'])\n",
    "cmap_bold = ListedColormap(['#FF0000', '#00FF00', '#0000FF'])\n",
    "plt.figure(figsize=(10,5))\n",
    "plt.pcolormesh(xx, yy, Z, alpha=.6,cmap=cmap_light)\n",
    "plt.title('PCA - Iris dataset')\n",
    "plt.xlabel('Dimension 1')\n",
    "plt.ylabel('Dimension 2')\n",
    "for xy,c in zip(X_selected,df_train[\"label\"]):\n",
    "    plt.scatter(xy[0],xy[1],c= 'red' if c == 0 else 'blue')\n",
    "plt.show()\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "from model.features import f_importances_graph"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "slowa_anty, slowa_pro = f_importances_graph(clf.coef_,tfidf_tokens, limit=50)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "with open(\"model_storage/words_pa.pkl\",\"rb\") as file:\n",
    "    words_pa = pickle.load(file)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# wektory artykułów\n",
    "from collections import Counter,defaultdict\n",
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "\n",
    "slowa_kluczowe = [s for _,s in slowa_pro+slowa_anty]\n",
    "article_vectors = []\n",
    "\n",
    "\n",
    "for a in words_pa:\n",
    "    zliczone = Counter([word for word in a if word in slowa_kluczowe])\n",
    "    article_vectors.append([zliczone[s] for s in slowa_kluczowe])\n",
    "\n",
    "article_vectors = TfidfTransformer().fit_transform(article_vectors)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "raw",
   "source": [
    "\n",
    "\n",
    "from sklearn_som.som import SOM\n",
    "\n",
    "som = SOM(100,1,100,max_iter=5000)\n",
    "\n",
    "som.fit(article_vectors.toarray(),10000)\n",
    "\n",
    "p = som.predict(article_vectors.toarray())\n",
    "\n",
    "\n",
    "p,c = zip(*zip(p,[\"g\" if l else \"r\" for l in df_train[\"label\"]]))\n",
    "\n",
    "\n",
    "plt.scatter(range(len(p)),p,c=c)\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% raw\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# wektory słów od słów dla każdego artykułu\n",
    "\n",
    "#print(prepare_dictionary(words_pa,slowa_anty+slowa_pro).keys())\n",
    "\n",
    "r = 2\n",
    "words = [w for _,w in slowa_anty+slowa_pro]\n",
    "slowa = words\n",
    "bliskie_slowa = set()\n",
    "for a in words_pa:\n",
    "    for i, w in enumerate(a):\n",
    "        if w in slowa:\n",
    "            for n in range(max(i - r, 0), min(i + r, len(a))):\n",
    "                if a[n] not in slowa:\n",
    "                    bliskie_slowa.add(a[n])\n",
    "\n",
    "bliskie_slowa = list(bliskie_slowa)\n",
    "matrix_pro = []\n",
    "matrix_anty = []\n",
    "for article_index,article in enumerate(words_pa):\n",
    "    if df_train[\"label\"][article_index] == 0:\n",
    "        slowa = [sl for _,sl in slowa_anty]\n",
    "        matrix = matrix_anty\n",
    "    else:\n",
    "        slowa = [sl for _,sl in slowa_pro]\n",
    "        matrix = matrix_pro\n",
    "    kluczowe_w_artykule = [s for s in slowa if s in article]\n",
    "    matrix.append([np.zeros(len(bliskie_slowa)) for s in kluczowe_w_artykule])\n",
    "    for word_index,s in enumerate(article):\n",
    "        if s in slowa:\n",
    "            for n in range(max(word_index - r, 0), min(word_index + r, len(article))):\n",
    "                if article[n] not in words:\n",
    "                    matrix[-1][kluczowe_w_artykule.index(s)][bliskie_slowa.index(article[n])] += 1"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "for i in range(len(matrix_pro)):\n",
    "    matrix_pro[i] = TfidfTransformer().fit_transform(matrix_pro[i]).toarray()\n",
    "\n",
    "for i in range(len(matrix_anty)):\n",
    "    matrix_anty[i] = TfidfTransformer().fit_transform(matrix_anty[i]).toarray()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "slowa = {sl: {s: 0 for s in bliskie_slowa} for _,sl in slowa_pro+slowa_anty}\n",
    "for article_index,article in enumerate(words_pa):\n",
    "    for word_index,s in enumerate(article):\n",
    "        if s in slowa:\n",
    "            for n in range(max(word_index - r, 0), min(word_index + r, len(article))):\n",
    "                if article[n] not in words:\n",
    "                    slowa[s][article[n]] += 1\n",
    "\n",
    "vectors = TfidfTransformer().fit_transform([list(w.values()) for w in slowa.values()])"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "raw",
   "source": [
    "\n",
    "from minisom import MiniSom\n",
    "import os\n",
    "\n",
    "def zwolennik(wiedza,to_czytania,opis=\"czlek\"):\n",
    "\n",
    "    current_directory = os.getcwd()\n",
    "    final_directory = os.path.join(current_directory, f'plots/{opis}')\n",
    "    if not os.path.exists(final_directory):\n",
    "        os.makedirs(final_directory)\n",
    "\n",
    "    som_zwolennik = MiniSom(10, 10, len(bliskie_slowa), sigma=3, learning_rate =0.05, neighborhood_function='gaussian')\n",
    "    learn_articles = np.random.choice(wiedza,size=int(len(wiedza)*0.8),replace=False)\n",
    "    print(\"Wiedza\")\n",
    "    for i,article in enumerate(learn_articles):\n",
    "        som_zwolennik.train_random(article, 1)\n",
    "\n",
    "    print(\"Nauka\")\n",
    "    plots = []\n",
    "    slowa_kluczowe = [s for _,s in slowa_pro+slowa_anty]\n",
    "    colors = ['r' if i>len(slowa_pro) else 'g' for i,_ in enumerate(slowa_kluczowe)]\n",
    "    for i,article in enumerate(matrix_pro):\n",
    "        x = plt.figure(figsize=(20, 20))\n",
    "\n",
    "        for _, (t, c, vec) in enumerate(zip(slowa_kluczowe, colors, vectors.toarray())):\n",
    "            winnin_position = som_zwolennik.winner(vec)\n",
    "            plt.text(winnin_position[0], winnin_position[1]+np.random.rand()*.9,t,fontsize=30,color=c)\n",
    "\n",
    "        plt.xticks(range(10))\n",
    "        plt.yticks(range(10))\n",
    "        plt.grid()\n",
    "        plt.xlim([0, 10])\n",
    "        plt.ylim([0, 10])\n",
    "        plt.plot()\n",
    "        plt.savefig(f\"plots/{opis}/w{i}.png\")\n",
    "        plots.append(f\"plots/{opis}/w{i}.png\")\n",
    "        plt.close(x)\n",
    "\n",
    "        som_zwolennik.train_random(article,1)\n",
    "    print(\"Gif\")\n",
    "    with imageio.get_writer(F'plots/{opis}/plots.gif', mode='I') as writer:\n",
    "        for filename in plots:\n",
    "            image = imageio.imread(filename)\n",
    "            writer.append_data(image)\n",
    "\n",
    "\n",
    "zwolennik(matrix_anty,matrix_pro,\"anty_czyta_pro\")\n",
    "zwolennik(matrix_pro,matrix_anty,\"pro_czyta_anty\")\n",
    "zwolennik(matrix_anty+matrix_pro,matrix_pro,\"random_czyta_pro\")\n",
    "zwolennik(matrix_anty+matrix_pro,matrix_anty,\"random_czyta_anty\")\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% raw\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "lista przeczytanych artykułów\n",
    "na bierząco generowane wektory ze wszystkich\n",
    "przeczytanych artykułów\n",
    "wektory mają długość całego sąsiedztwa słów\n",
    "\n",
    "learnig_rate\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "from minisom import MiniSom\n",
    "from random import sample\n",
    "from preprocessing.vectors import prepare_dictionary,generate_vectors, find_close_words\n",
    "\n",
    "def random_percent(data,percent):\n",
    "    if percent<=1:\n",
    "        return sample(data,k=int(len(data)*percent))\n",
    "    return None\n",
    "\n",
    "key_words = [s for _,s in slowa_pro + slowa_anty]\n",
    "close_words = find_close_words(words_pa,key_words)\n",
    "pro_words_pa = [words for i,words in enumerate(words_pa) if df_train[\"label\"][i]==1]\n",
    "anti_words_pa = [words for i,words in enumerate(words_pa) if df_train[\"label\"][i]==0]\n",
    "\n",
    "def plot_vectors(som,vectors,N):\n",
    "    x = plt.figure(figsize=(20, 20))\n",
    "\n",
    "    for _, (t, c, vec) in enumerate(zip(key_words, ['r' if i>len(slowa_pro) else 'g' for i,_ in enumerate(slowa_kluczowe)], list(vectors.values()))):\n",
    "        winnin_position = som.winner(vec)\n",
    "        plt.text(winnin_position[0], winnin_position[1]+np.random.rand()*.9,t,fontsize=30,color=c)\n",
    "\n",
    "    plt.xticks(range(N))\n",
    "    plt.yticks(range(N))\n",
    "    plt.grid()\n",
    "    plt.xlim([0, N])\n",
    "    plt.ylim([0, N])\n",
    "    plt.plot()\n",
    "    plt.show()\n",
    "    plt.close(x)\n",
    "\n",
    "def sim(wiedza,do_czytania,learning_rate):\n",
    "    # wiedza na start\n",
    "    read_data = random_percent(wiedza,0.8)\n",
    "    vectors = generate_vectors(prepare_dictionary(read_data,key_words,r=2),close_words)\n",
    "\n",
    "    N = int(5*np.sqrt(len(vectors)))\n",
    "    som = MiniSom(N, N, len(close_words), sigma=3, learning_rate=learning_rate, activation_distance='cosine')\n",
    "    som.train_random(list(vectors.values()),500,verbose=True)\n",
    "\n",
    "    plot_vectors(som,vectors,N)\n",
    "\n",
    "    read_data.extend(random_percent(do_czytania,0.5))\n",
    "    vectors = generate_vectors(prepare_dictionary(read_data,key_words,r=2),close_words)\n",
    "\n",
    "    som.train_random(list(vectors.values()),500,verbose=True)\n",
    "\n",
    "    plot_vectors(som,vectors,N)\n",
    "\n",
    "sim(anti_words_pa,pro_words_pa,0.5)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}